To provide a highly personalized, timely, and digestible stream of critical news and insights across a vast array of topics relevant to working, investing, and building businesses in Africa, specifically the DRC. The platform must leverage advanced scraping, AI summarization, and content generation, delivered through a modern web and mobile experience.
Target User: Young, passionate tech engineers and business professionals from the African diaspora (especially Congolese) interested in engaging with Africa.
Core Requirements & Features:
1. Data Ingestion & "Deep Scraper" (Data Extraction & Exfiltration): * Scope: The scraper must be designed to "scrape the whole internet," meaning it needs a highly adaptable and comprehensive strategy to gather information from: * Standard Websites: News portals, blogs, corporate websites, government publications (using Beautiful Soup, Scrapy). * Social Media Platforms: Public posts, trends, discussions from X (Twitter), LinkedIn, Facebook, Instagram (using platform-specific APIs where available, or advanced scraping techniques where public data permits, strictly adhering to platform ToS and ethical guidelines). 

* Mobile Apps & Desktop Apps (Indirect): Where direct scraping is impossible, identify and scrape associated web versions of their content, press releases, app store descriptions, or news/reviews about them. 
* Local & Niche Sources: Actively target and prioritize a continuously updated list of local African news sites (DRC: Actualite.cd, Radio Okapi; regional: Jeune Afrique, TechCabal, AfDB, World Bank Africa reports, specific national government portals, etc.). 
* Topic Coverage: Systematically scrape based on the entire "Extended List of Topics for Your Intelligence Platform" provided, which is granularly broken down into Macro Environment, Technology & Venture Ecosystem, and People, Talent & Cultural Nuances. This includes specific keywords, source types, and geographical filters (DRC-centric primarily, then broader Africa). 
* Data Extraction: Extract key elements: original URL, headline, full text content, publication date/time, source name, authors, relevant tags/categories. 
* Exfiltration: Securely store extracted raw data in a temporary data lake (e.g., S3) for processing. 
* Frequency: The scraping process must run daily (or more frequently for high-priority/breaking news topics) as an automated background task. 
* Ethical Considerations: Implement robust robots.txt adherence, respect rate limits, and clearly state data sources. Avoid scraping protected or private user data.
2. Intelligent Content Processing & Generation: * AI Summarization: For every scraped article/piece of content, generate a catching (click-baity) headline and a a concise, factual, 2-3 sentence summary using a fine-tuned or high-quality pre-trained LLM (e.g., Hugging Face Transformers: BART/T5 or a similar model suitable for deployment on AWS Lambda/SageMaker). * Content Generation: Based on the scraped data and AI summaries, automatically generate: * Short Videos (Script + Visuals): * Script: A concise script (approx. 30-60 seconds) highlighting the key points of the most important stories, incorporating the AI summaries. * Visuals: Suggest dynamic stock footage, relevant images, or simple animations that align with the news, to be compiled into a video format (the agent will generate the script and describe the visuals, not render the actual video, which would be a separate service). * Short Blog Posts: A brief blog post format (approx. 500-700 words) for each key story, starting with the AI summary, adding context, and then linking to the original source. * Timeliness: All generated content (videos, blog posts) must explicitly include the date and time of generation/reporting to emphasize its timeliness. * Original Source Linkage: Crucially, every piece of generated content (video description, blog post) must include a clear, direct link to the original source article/content. This maintains integrity and allows users to deep-dive.
3. Frontend Experience (Web & Mobile Apps): * Web App (Main Platform): * Modern & Simplistic Look: Clean UI/UX, easy navigation, professional aesthetic. * "Daily News" Section (Homepage): Displays the generated content. * Initial View: Top 5 most relevant stories (either short videos or blog posts). 
* Infinite Scroll/Pagination: To browse more stories. 
* Filters: Allow users to filter by category (e.g., "Fintech," "Infrastructure," "DRC Specific"), date, and relevance score. 
* Search: Full-text search capability. * User Profile & Preferences: * Personalization: Users can define their preferred topics, regions (e.g., "DRC only," "West Africa"), and content types (e.g., "prioritize videos"). * Device Context: The backend API must intelligently adapt the number of stories displayed to fit the "medium that the user is using" (e.g., fewer, highly summarized items for a small mobile widget vs. more detailed lists for a laptop/TV). 
* Mobile App (React Native with Expo): * Cross-Platform: Single codebase for iOS and Android. 
* Native Widget Layer: Implement iOS (WidgetKit) and Android (App Widgets) widgets to display "Top 3 Daily News" summaries directly on the user's home screen based on their preferences. 
* Core App Features: Replicate the "Daily News" section from the web app, with full filtering, search, and content consumption capabilities. 
* User Authentication: Seamless login/registration experience.
4. Backend Architecture (Python-focused with FastAPI/Flask): * API Development: Build a robust RESTful API with FastAPI/Flask for all frontend interactions (content delivery, user management, preferences). * Data Processing Pipeline: * Scraping Orchestration: Celery + Redis for managing scheduled scraping tasks. * Data Transformation: Pandas + NumPy for cleaning, normalizing, and enriching scraped data. * Relevance Scoring: Implement a basic algorithm to score the relevance of articles based on user preferences, keywords, and publication authority. * User Management: Secure user registration, authentication (JWT), and profile management. * Content Management: APIs for CRUD operations on scraped articles, generated content (summaries, video scripts, blog posts). * Personalization Engine: Logic to filter and sort content based on stored user preferences and device type.
5. AWS Infrastructure (Terraform-managed): * Compute: * ECS Fargate: For containerized FastAPI/Flask backend API and Celery workers. * AWS Lambda: For event-driven tasks like triggering scraping jobs, initial AI summarization, or small utility functions. * Database: Amazon RDS (PostgreSQL) for all structured data (users, articles, content, preferences). Use PostGIS if location-based filtering becomes a future feature. * Caching: ElastiCache (Redis) for Celery message broker and API caching to improve performance. * Storage: Amazon S3 for raw scraped data, generated content (video scripts, blog post text), and static frontend assets. * Networking & Delivery: * API Gateway: Fronting the ECS Fargate service for scalable and secure API access. * CloudFront: For content delivery (CDN) for frontend assets and potentially generated video content. * VPC, Subnets, Security Groups: Secure network isolation. * Route 53: DNS management for the web application. * Security: * AWS WAF: To protect against common web exploits. * GuardDuty: For intelligent threat detection. * Secrets Manager: Securely store API keys (social media, scraping services, AI models) and database credentials. * IAM: Granular access control for all AWS resources. * Scalability: Implement auto-scaling groups for ECS Fargate services based on demand metrics.
6. DevOps Integration (GitHub Actions & Docker): * Containerization: Dockerize all backend services (FastAPI/Flask, Celery worker, Scrapy spiders). * CI/CD Pipeline (GitHub Actions): Automate: * Code Linting & Testing (unit, integration). * Docker Image Builds & Pushes to ECR (Elastic Container Registry). * Terraform Plan & Apply for infrastructure changes. * Blue/Green Deployments for zero-downtime updates to ECS Fargate services.
7. Monitoring & Logging (Splunk Integration): * Log Collection: Configure Splunk Universal Forwarders (or integrate directly from CloudWatch/Firehose) to collect all application logs (FastAPI, Celery), AWS service logs (CloudTrail, VPC Flow Logs, API Gateway logs), and system metrics. * Security Monitoring: Develop custom Splunk apps/dashboards to monitor for security anomalies, scraping failures, and system health. * Alerting: Configure alerts for critical events (e.g., API errors, service downtime, security incidents).
Deliverables:
1. Complete Source Code:
    * Full Python backend (FastAPI/Flask, scraping scripts, AI processing).
    * React Native with Expo mobile app (including widget implementations).
    * Web app frontend (HTML/CSS/JS or lightweight framework like Vue/React).
    * Dockerfiles for all services.
2. Terraform Scripts: Complete .tf files to provision all AWS infrastructure components described.
3. Deployment & Operations Guide: A comprehensive README.md and DEPLOYMENT.md detailing:
    * Project structure.
    * Local setup instructions.
    * AWS prerequisites and configuration (IAM roles, API keys).
    * Step-by-step deployment process using Terraform and GitHub Actions.
    * Instructions for setting up Splunk integration.
    * Guidelines for adding new scraping sources and refining AI models.
4. Security Best Practices: Ensure all code and infrastructure configurations adhere to industry-standard cybersecurity best practices, especially concerning data privacy (GDPR/CCPA compliance considerations).
Execution Strategy:
* Start by defining the database schema based on the required data types.
* Develop the core scraping and data ingestion pipeline first, ensuring it can handle the specified topics.
* Implement the AI summarization and content generation (video script, blog post text) as separate, modular services.
* Build the FastAPI/Flask backend to serve this processed content and manage user preferences.
* Develop the web frontend and then the React Native mobile app (including widgets), ensuring they consume the backend APIs effectively and adapt content display based on user preferences and device type.
* Provision all infrastructure incrementally using Terraform and set up the CI/CD pipeline early.
* Integrate monitoring and logging throughout the development process.